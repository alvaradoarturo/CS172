crawler 
-run in parralel, deal with duplication

duplicate detection
- fingerprint example L7 s7
- simhash example L7 s19, know how to use algorithim


1. look at center of every cluster and find point in the center
	minimize the sum of squares disitance between this point and each point in 
	a cluster to obtain a good cluster
	
2. choosing a further poibt as cluster center, seed sensitve to outliers
   choosing the number of clusters dont know k,
		- deal with by: trial and error to find k, check sum of squared error value

3. step 1
	cluster 1: A
	cluster 2: B, C, D, E
	
   step 2
	cluster 1 centroid: 0.1
	cluster 2 centroid: take aveage of values = 1.6
	
   step 3
   cluster 1: A, B, C
   cluster 2: D, E
   
   Step 4
   cluster 1 centroid: 
   cluster 2 centroid:
   
   step 5
   cluster 1: 
   cluster 2: 
   
   know stopping points ex, cluster do not change
   
4. choosing seed urls to get enougth coverage
	respectuful of website rules - not hitting domain too much
	detecting duplicates

5. 

6. 
	
	
   