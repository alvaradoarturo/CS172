# CS172

## Part 1 - Crawler
Overview of system, 
(a) Architecture
Our Architecture of the system reads in a file of seed of URLs and we crawl the .edu pages. 
We make the crawler so its able to crawl these different ypes of .edu pages. 
In our application it also shows the number of pages that we crawl to and what the levels are. All the crawled page that we crawled gets stored in a folder. 

(b) The Crawling or data collection strategy (do you handle duplicate URLs, is your crawler parallel, etc.)
For our crawler we are able to handle the duplicate urls. 
(c) Data Structures employed
Instruction on how to deploy the crawler. Ideally, you should include a crawler.bat (Windows) or crawler.sh (Unix/Linux) executable file that takes as input all necessary parameters. Example instructions for Web-based assignment: [user@server]./crawler.sh < seed − Fileseed.txt > < num − pages : 10000 > < hops − away : 6 > <output−dir >
## Part 2 - Indexer
Instructions on how to deploy the system. Ideally, you should include an indexer.bat (Windows) or indexer.sh (Unix/Linux) executable file that takes as input all necessary parameters .  Example: [user@server] ./indexer.sh < output − dir >


## Part 3 - Extension
Detailed description of your ‘extension’ and motivation or benefit of the implemented feature or extension. Include screen shots of your system in action.
